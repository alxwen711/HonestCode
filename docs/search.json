[
  {
    "objectID": "wip/isitrng.html",
    "href": "wip/isitrng.html",
    "title": "isitrng",
    "section": "",
    "text": "library(cowplot)\nlibrary(infer)\nlibrary(knitr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(devtools)\n\nLoading required package: usethis\n\nlibrary(testthat)\n\n\nAttaching package: 'testthat'\n\n\nThe following object is masked from 'package:devtools':\n\n    test_file\n\n\nThe following object is masked from 'package:purrr':\n\n    is_null\n\n\nThe following object is masked from 'package:dplyr':\n\n    matches\n\n\n\nplacement &lt;- tibble(rank = c(1,1,2,3,3,4,4,4,4,4,4,4,4))\n\n\nboot &lt;- rep_sample_n(placement,size=13,reps=100000,replace = TRUE) |&gt;\n  group_by(replicate) |&gt;\n  summarise(mean = mean(rank))\nboot\n\n# A tibble: 100,000 × 2\n   replicate  mean\n       &lt;int&gt; &lt;dbl&gt;\n 1         1  3.69\n 2         2  3.62\n 3         3  3.62\n 4         4  2.85\n 5         5  3.62\n 6         6  3.62\n 7         7  3.23\n 8         8  3.62\n 9         9  3.85\n10        10  3.31\n# ℹ 99,990 more rows\n\n\n\nci &lt;- get_confidence_interval(boot, level = 0.99)\nci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     2.38     3.92\n\n\n\nbootstrap_dist_100_plot &lt;- boot |&gt;\n  ggplot(aes(x = mean)) +\n  geom_histogram(bins=length(unique(boot$mean))) + \n  geom_vline(\n    xintercept = 2.5,\n    color = \"red\", linewidth = 1.5\n  ) +\n  geom_vline(\n    xintercept = 2.288,\n    color = \"green\", linewidth = 1.5\n  ) +\n  annotate(\"rect\",\n    xmin = ci$lower_ci[[1]], xmax = ci$upper_ci[[1]], ymin = 0, ymax = Inf,\n    fill = \"deepskyblue\",\n    alpha = 0.3\n  ) +\n  xlab(\"Placement Mean\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap Sample Mean Distribution\")\n\n# YOUR CODE HERE\n \nbootstrap_dist_100_plot"
  },
  {
    "objectID": "wip/isitrng.html#r-markdown",
    "href": "wip/isitrng.html#r-markdown",
    "title": "isitrng",
    "section": "",
    "text": "library(cowplot)\nlibrary(infer)\nlibrary(knitr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(purrr)\nlibrary(ggplot2)\nlibrary(devtools)\n\nLoading required package: usethis\n\nlibrary(testthat)\n\n\nAttaching package: 'testthat'\n\n\nThe following object is masked from 'package:devtools':\n\n    test_file\n\n\nThe following object is masked from 'package:purrr':\n\n    is_null\n\n\nThe following object is masked from 'package:dplyr':\n\n    matches\n\n\n\nplacement &lt;- tibble(rank = c(1,1,2,3,3,4,4,4,4,4,4,4,4))\n\n\nboot &lt;- rep_sample_n(placement,size=13,reps=100000,replace = TRUE) |&gt;\n  group_by(replicate) |&gt;\n  summarise(mean = mean(rank))\nboot\n\n# A tibble: 100,000 × 2\n   replicate  mean\n       &lt;int&gt; &lt;dbl&gt;\n 1         1  3.69\n 2         2  3.62\n 3         3  3.62\n 4         4  2.85\n 5         5  3.62\n 6         6  3.62\n 7         7  3.23\n 8         8  3.62\n 9         9  3.85\n10        10  3.31\n# ℹ 99,990 more rows\n\n\n\nci &lt;- get_confidence_interval(boot, level = 0.99)\nci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     2.38     3.92\n\n\n\nbootstrap_dist_100_plot &lt;- boot |&gt;\n  ggplot(aes(x = mean)) +\n  geom_histogram(bins=length(unique(boot$mean))) + \n  geom_vline(\n    xintercept = 2.5,\n    color = \"red\", linewidth = 1.5\n  ) +\n  geom_vline(\n    xintercept = 2.288,\n    color = \"green\", linewidth = 1.5\n  ) +\n  annotate(\"rect\",\n    xmin = ci$lower_ci[[1]], xmax = ci$upper_ci[[1]], ymin = 0, ymax = Inf,\n    fill = \"deepskyblue\",\n    alpha = 0.3\n  ) +\n  xlab(\"Placement Mean\") +\n  ylab(\"Frequency\") +\n  ggtitle(\"Bootstrap Sample Mean Distribution\")\n\n# YOUR CODE HERE\n \nbootstrap_dist_100_plot"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HonestCode Main Page",
    "section": "",
    "text": "Rule of 100 Million: Estimating Big-O Runtime Expectations\n\n\n\ncompetitive programming\n\n2026\n\n\n\n\n\n\n\n\n\nJan 24, 2026\n\n\nAlexander Wen\n\n\n\n\n\n\n\n\n\n\n\n\nAn Intro to Segment Trees and Their Applicability to Database Management Systems\n\n\n\n2026\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 16, 2026\n\n\nAlexander Wen\n\n\n\n\n\n\n\n\n\n\n\n\nSparse Tables\n\n\n\ndata structures\n\n2026\n\n\n\n\n\n\n\n\n\nJan 8, 2026\n\n\nAlexander Wen\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/rule-of-100-million/index.html",
    "href": "posts/rule-of-100-million/index.html",
    "title": "Rule of 100 Million: Estimating Big-O Runtime Expectations",
    "section": "",
    "text": "There is a reason why experienced competitive programmers can look at a problem and then within seconds, be able to determine a basic idea of the solution. All problems in competitive programming require an understanding of how fast an algorithm must be to solve it, and within each problem statement there is important snippets that can immediately help you perform a basic time complexity analysis."
  },
  {
    "objectID": "posts/rule-of-100-million/index.html#a-quick-review-of-big-o-notation",
    "href": "posts/rule-of-100-million/index.html#a-quick-review-of-big-o-notation",
    "title": "Rule of 100 Million: Estimating Big-O Runtime Expectations",
    "section": "A Quick Review of Big-O Notation",
    "text": "A Quick Review of Big-O Notation\nTime complexity is represented through Big-O Notation, which in mathematical terms, states that a function \\(f(x)\\) is equal to \\(O(g(x))\\) if and only if\n\\[\n\\exists M,x_0 \\text{ s.t. } |f(x)| \\leq Mg(x), \\forall x \\geq x_0\n\\]\nTo explain this more simply, an algorithm has a time complexity of \\(O(f(n))\\) if for some arbritary minimal input size, when increasing the input size by a factor of \\(n\\), the runtime of the algorithm increases by a factor of \\(f(n)\\). In the context of competitive programming, the runtime is usually approximated by the number of basic operations performed, which are general computer operations (adding, subtracting, conditionals) that have a near instant and constant \\(O(1)\\) amount time to complete. (Technically running time.sleep(100) in Python would be a \\(O(1)\\) operation since it will always pause the program for 100 seconds, but it can hardly be called instantaneous, so while it is a constant operation, it is not basic.) Basic operations can also include array index lookup, adding two numbers, modulo division, and assigning variables.\n\n\n\n\n\n\nNote\n\n\n\nDictionary additions and lookups can also be considered basic operations, but this assumes there are an insignifcant number of collisions in the hashing function. Especially within CodeForces there are scenarios where users can design test cases to intentionally cause many such collisions resulting in set/dict lookup to take \\(O(n)\\) time per operation. https://codeforces.com/blog/entry/62393 contains a very robust setup at the end to make a custom hash that is immune to such hacks, but from practice, even small adjustments to the basic hash function used are usually enough.\n\n\nNow that this overview has been taken care of, the rule of 100 million can be summed up by the following subtitle:"
  },
  {
    "objectID": "posts/rule-of-100-million/index.html#assume-at-most-100-million-operations-per-second",
    "href": "posts/rule-of-100-million/index.html#assume-at-most-100-million-operations-per-second",
    "title": "Rule of 100 Million: Estimating Big-O Runtime Expectations",
    "section": "Assume at most 100 Million Operations Per Second",
    "text": "Assume at most 100 Million Operations Per Second\nFormally, to estimate runtime, we can assume that a program can complete up to 100 million basic operations per second.\nIn practice, assuming a program will perform 100 million operations per second is fairly optimistic. Most problems are designed to allow quite a bit of leniency up to roughly a factor of 3, so assuming about 30 million operations per second for C++ is a safe estimate. As for Python, this number drops to around 10 million operations per second, but having done many contests using Python, the number of times this actually affected me where an algorithm in C++ passes but that same one in Python fails is maybe once.\nKnowing this, let’s use a few example statements to estimate the maximum runtime complexity allowed:\n\n\n\nSource: Global Round 27 C (https://codeforces.com/contest/2035/problem/C)\n\n\nHere we observe that the maximum value of \\(n\\) is 200000, and that we have 2 seconds in this problem. \\(200000^2 = 4*10^{10} &gt; 2*10^8\\), so we can safely assume an \\(O(n^2)\\) algorithm is not going to work. On the other hand, \\(200000 &lt; 2*10^8\\), so we can be very sure that an \\(O(n)\\) algorithm can work. The tightest possible bound here would be \\(O(n \\log^2 n)\\); computationally this works out to about \\(\\log(200000)^2*200000 \\approx\\) 62 million operations, which is under the 100 million operation per second assumption, but using 30 million operations per second as a safe C++ or optimistic Python estimate makes this complexity iffy. In actual practice, \\(O(n \\log^2 n)\\) complexities are quite rare and this problem would most likely be expecting a time complexity of \\(O(n \\log n)\\) or better given these constraints.\n\n\n\nSource: Global Round 27 F (https://codeforces.com/contest/2035/problem/F)\n\n\nIn this case, the maximum value of \\(n\\) is 2000, and that we have 4 seconds in this problem. This time a \\(O(n^2)\\) algorithm will be possible because \\(2000^2 = 4*10^{6} &lt; 4*10^8\\). It is also worth noting that usually Codeforces problems have 1 or 2 second time limits, in the rare cases where a problem has 4 seconds allotted to it, it usualy means higher time complexities can be possible; in this case, \\(O(n^2 \\log n)\\) (roughly 44 million) and even \\(O(n^{2.5})\\) (roughly 180 million) are possible time complexities for this solution. Especially on harder problems like this one, these are cases where more complex runtime algorithms can be possible, as the intended solution does have a \\(O(n^2 \\log n)\\) solution.\nBelow is a table of many of the more common runtime possibilities, and a rough estimate for the maximum input size \\(n\\) for which an algorithm should reasonably be able to pass in 1 second:\n\n\n\n\n\n\n\n\nTime Complexity\nMaximum n per second\nExamples\n\n\n\n\nO(1), O(log n)\n\\(n \\\\leq 10^{18}\\)\nBasic operations, binary search\n\n\nO(sqrt n)\n\\(n \\\\leq 10^{14}\\)\nTrial division primality test\n\n\nO(n)\n\\(n \\\\leq 10^7\\)\nMaximum of an array\n\n\nO(n log n)\n\\(n \\\\leq 10^6\\)\nSorting algorithms\n\n\nO(n log^2 n)\n\\(n \\\\leq 200000\\)\nQuery type problems on segment tree\n\n\nO(n sqrt n)\n\\(n \\\\leq 50000\\)\nSqrt decomposition, Mo’s algorithm\n\n\nO(n^2)\n\\(n \\\\leq 3000\\)\nBrute force subarray sums\n\n\nO(n^2 log n)\n\\(n \\\\leq 1000\\)\nNiche dp cases, very rare\n\n\nO(n^3)\n\\(n \\\\leq 300\\)\nFloyd-Warshall\n\n\nO(n^4)\n\\(n \\\\leq 60\\)\n4D DP\n\n\nO(2^n)\n\\(n \\\\leq 25\\)\nBrute force on all subsets\n\n\nO(n!)\n\\(n \\\\leq 11\\)\nBrute force on all permutations\n\n\n\nThis table is not comprehensive, but covers just about every time complexity I have observed. In actuality, the bolded ones are the most common time complexities used in problems and are thus the most important to remember."
  },
  {
    "objectID": "posts/rule-of-100-million/index.html#exceptions-where-slowest-allowed-fastest-possible",
    "href": "posts/rule-of-100-million/index.html#exceptions-where-slowest-allowed-fastest-possible",
    "title": "Rule of 100 Million: Estimating Big-O Runtime Expectations",
    "section": "Exceptions Where Slowest Allowed != Fastest Possible",
    "text": "Exceptions Where Slowest Allowed != Fastest Possible\nThe above table can be used accurately for the slowest possible algorithms a problem will accept. For the most part, this table also provides a decent estimation of what the time complexity actually is, but there are cases where a significantly faster solution is possible than what the input size implies.\n\nCodeforce Div 2/3/4 Problem A\nThe point of a problem A on a Codeforces contest is so that everyone (or least almost everyone) is able to solve at least one problem on a Codeforces contest. In divison 1 contests, Problem A can be fairly challenging and thus have a reasonable time complexity estimation by it’s maximum allowable algorithm, but otherwise, many of these problems are created without consideration of optimizing the time complexity of the solution.\n\n\n\nSource: Codeforces Round 1074 Div. 4 A (https://codeforces.com/contest/2185/problem/A)\n\n\nIn this case, we notice that the maximum \\(n\\) value in a testcase is 20. Does that mean that the solution to this introductory problem has a \\(O(2^n)\\) time complexity? Well you could make a solution that does, but obviously there’s faster! If we read the question carefully, we can conclude that every positive integer is a perfect root because \\(\\sqrt{x^2} = x\\) and \\(x^2\\) uniquely exists for every positive integer. So all we’d have to do is output \\(n\\) unique numbers for each testcase, which is an \\(O(n)\\) algorithm.\n\n\n\\(O(n^3)\\) Estimates and Longer\nCases where the estimated runtime can be at least \\(O(n^3)\\) usualy mean the runtime is this long, especially on harder problems. But there are cases where the solution can be surprisingly faster, as one of these cases occurred to me recently to inspire the creation of this post.\n\n\n\nSource: Codeforces Round 1073 Div. 1 B2 (https://codeforces.com/contest/2190/problem/B2)\n\n\nWith this case, a time complexity as high as even \\(O(n^4)\\) is possible because with a maximum \\(n\\) value of 100, \\(100^4 = 10^8 &lt; 2 * 10^8\\) for a 2 second problem. Furthermore, from my initial attempts on this problem using a dynamic programming approach, I roughly computed that there was closer to around 12.5 million basic operations needed when \\(n = 100\\), so even if I was using Python, a \\(O(n^4)\\) idea was possible, and I assumed that the solution for this problem really was no faster than \\(O(n^4)\\).\nWhat then proceeded to follow was 14 Time Limit Exceeded verdicts of complete and total agony as this contest dropped me out of the Candidate Master ranking on Codeforces. Here is a link to my last solution with a O(n^4) attempt where you can try and understand what exactly I was thinking..\nFrom the editorial, there is a \\(O(n^3)\\) approach to this that can be optimized further to the \\(O(n^2)\\) algorithm I eventually submitted for upsolving, showing that the input limits are only absolute in determining the slowest possible solution, and not the actual fastest possible solution. I won’t explain the reasoning too extensively as I don’t want to provide spoilers for the problem and the editorial is effective for explaning the solution, but as for an exercise for the reader, try to understand how the observations made in my slow \\(O(n^4)\\) solution can be simplified greatly to create an \\(O(n^3)\\) or better algorithm.\n\n\n\n\n\n\nHow Can My O(n^4) Solution Be Faster? (Spoilers for Round 1073 Problem B2)\n\n\n\n\n\nThe main slowdown is the dp setup used, where I am tracking the last and second last occurances of (, the first occurance of ), the count of ( - count of ), and the length of the bracket subsequence being considered. Furthermore, I observe that the bracket sequence is valid (ie. the score is not 0) only if the count of ( - count of ) is 0 AND the second last occurrance of ( is after the first occurance of ). This means this complex occurance tracking nonsense can be simplified to checking if the subsequence )(( exists the created subsequence or not.\nTo tell if )(( exists, all that needs to be done is to create a counter x which starts at 0. If a ) is added to the subsequence and x is 0, then x is increased to 1. If ( is added to the subsequence and x is 1 or 2, x is increased by 1. Otherwise, x remains as is. Then if this counter x ends up at 3, then the subsequence )(( exists. This means the only things needed to track in the dp are this counter, the count of ( - count of ), and the length of the bracket subsequence. This then makes a \\(O(n^3)\\) solution possible.\nFor the \\(O(n^2)\\) solution, another observation (which I somehow did end up making even in the \\(O(n^4)\\) attempts) is that the score of any bracket sequence will always be either 0 or \\(n-2\\), where \\(n\\) is the length of the sequence. This then means that only information needed to be tracked for computing the answer are the number of bracket sequences with the specific counter value and count of ( - count of ) and their combined lengths, which is possibe in \\(O(n^2)\\). My upsolved solution is an example of this. (https://codeforces.com/contest/2190/submission/358384748)"
  },
  {
    "objectID": "posts/database-segment-tree/index.html",
    "href": "posts/database-segment-tree/index.html",
    "title": "An Intro to Segment Trees and Their Applicability to Database Management Systems",
    "section": "",
    "text": "It goes without saying that PostgreSQL, MondoDB, Redis, and all other database management systems require efficient methods for managing and querying their stored databases. A similarity between nearly all of these database management systems is that they make use of one of the most flexible and powerful data structures: the segment tree."
  },
  {
    "objectID": "posts/database-segment-tree/index.html#a-segment-tree-example",
    "href": "posts/database-segment-tree/index.html#a-segment-tree-example",
    "title": "An Intro to Segment Trees and Their Applicability to Database Management Systems",
    "section": "A Segment Tree Example",
    "text": "A Segment Tree Example\nTo demonstrate the effectiveness of the segment tree, let’s use it in an example, that acts as a continuation from last week’s post about sparse tables. After John Code (our hiker from last week) maps out a mountain range and all of the various peaks, he ends up with a new set of data indicating the number of people that visited each landmark over the past week:\n\n\n\nMore mountain measurements.\n\n\nThese are the measurements expressed in an array:\n\n\n\nIndex #\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nNumber of tourists\n1\n2\n0\n3\n1\n4\n0\n2\n\n\n\nNow John is interested in which parts of the mountain range are the most popular. To do this, he queries the sum of various subarrays. For example, the subarray representing the part of the mountain range from landmark 3 to 5 saw \\(0 + 3 + 1 = 4\\) people visit last week, whereas the subarray representing the part of the mountain range from landmark 1 to 4 saw \\(1 + 2 + 0 + 3 = 6\\) people visit last week. He then records all of these measurements in a database, and now let’s see how such queries can be computed efficiently.\nTo create the segment tree, we are going to compute the sums for only some of the subarrays. First, we create a node for every single measurement made, representing the trivial sum values for every subarray of length 1:\n\n\n\nBase step of segment tree creation.\n\n\nNow we compute the sum values for the subarrays of length 2 by summing every pair of consecutive nodes like below:\n\n\n\nNode combining process\n\n\nThis new row now contains the sums for the subarrays with indexes going 1 to 2, 3 to 4, 5 to 6, and 7 to 8. Repeat this step with the new nodes created to get the sum values for subarrays of length 4:\n\n\n\nRepeating the previous step\n\n\nAnd now finally with the two nodes for the subarrays with indexes going 1 to 4 and 5 to 8, create the top layer of the segment tree representing the sum value for subarrays of length 8 (ie. the entire array):\n\n\n\nThe full segment tree.\n\n\nThis segment tree can now be used to compute the sum of any subarray by choosing nodes greedily, meaning that the nodes that cover the largest subarrays are chosen first to minimize the number of nodes involved. For instance, if the sum of the subarray from index 1 to 7 was to be computed:\n\n\n\nExample of the segment tree in use.\n\n\nThe yellow nodes indicate nodes that are only partially included in the subarray range. These nodes are not included in the sum but are instead searched deeper to their child nodes. The green nodes indicate nodes that are fully covered by the subarray range, so for maximum efficiency, these nodes are added to the sum and no further searching is required. For this example, the sum of the subarray from index 1 to 7 is \\(6 + 5 + 0 = 11\\).\nWith efficient node selection, at most one node from each layer is going to be selected. This means in a much larger case, if John Code’s robot made a million measurements in total, then because the number of nodes in each layer is divided by two each time, such a segment tree would have only 21 such layers, meaning that ANY subarray sum for an array of one million values can be computed by just adding at most 21 values together! A segment tree of that size would consist of \\(1000000 + 500000 + 250000 + ... + 2 + 1 \\approx 2000000\\) nodes, so while there is a substantial precomputation cost, if multiple subarray sums are required, this is a much faster method than burte forcing every query. Generally, for an array of \\(n\\) elements, a segment tree will create roughly \\(2n\\) nodes, and will require summing up at most \\(\\text{log}_2(n)\\) nodes to compute a subarray sum. In big-O notation, this means the amount of time to create a segment tree scales linearly with the size of the array and can be represented as \\(O(n)\\), while the time needed to complete a single query scales logarithmically with the size of the array and can be represented as \\(O(\\log n)\\)."
  },
  {
    "objectID": "posts/database-segment-tree/index.html#database-usability-of-segment-trees",
    "href": "posts/database-segment-tree/index.html#database-usability-of-segment-trees",
    "title": "An Intro to Segment Trees and Their Applicability to Database Management Systems",
    "section": "Database Usability of Segment Trees",
    "text": "Database Usability of Segment Trees\nTechnically, there are simpler methods to finding the sum of any subarray that are equally as fast or even faster, such as prefix sums. However, suppose that John later finds there was an error in his data, and that landmark 5 actually had a lot more people there last week, updating the array to something like below:\n\n\n\nIndex #\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nNumber of tourists\n1\n2\n0\n3\n100\n4\n0\n2\n\n\n\nThese data updates can be common in a database context, and what makes segment trees useful is how they handle these changes. First, we highlight all nodes that are affected by this change in the previous segment tree red. These nodes include the base node tracking the sum of the subarray from index 5 to 5 in the bottom layer, and each subsequent parent node up to the top.\n\n\n\nNodes that are affected by index 5 changing from 1 to 100.\n\n\nNow starting from the bottom node in this tree, update each node: - The sum of the subarray from index 5 to 5 is now 100 (previously 1) - The sum of the subarray from index 5 to 6 is now 104 (100 + 4) - The sum of the subarray from index 5 to 8 is now 106 (104 + 2) - The sum of the subarray from index 1 to 8 is now 112 (6 + 106)\n\n\n\nUpdated segment tree.\n\n\nNow with this updated segment tree, the subarray sum queries can be computed in the same way as before. Even with a much larger tree, the number of nodes that need to be updated will always be equal to the number of layers in the tree, thus meaning that the amount of time needed to perform such an update scales logarithmically with the array size. This means that this is roughly as fast as the \\(O(\\log n)\\) operation used for finding a subarray sum."
  },
  {
    "objectID": "posts/database-segment-tree/index.html#conclusion",
    "href": "posts/database-segment-tree/index.html#conclusion",
    "title": "An Intro to Segment Trees and Their Applicability to Database Management Systems",
    "section": "Conclusion",
    "text": "Conclusion\nIn short, segment trees are a data structure based solution that explains how database management systems are capable of processing database queries in an efficient manner. The subarray sum problem is just one of the many applicable uses of segment trees, and further explanation as well as a code implementation of the structure can be found on cp-algorithms, which is one of the largest information bases for various data structure and algorithmic concepts used in competitive programming."
  },
  {
    "objectID": "posts/sparse-table/index.html",
    "href": "posts/sparse-table/index.html",
    "title": "Sparse Tables",
    "section": "",
    "text": "Suppose a hiker is mapping out a mountain range. They walk across the mountain and record their current altitude in meters at important landmarks in this diagram:\n\n\n\nVarious mountain measurements.\n\n\nThese measurements can then be expressed in an array:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndex #\n1\n2\n3\n4\n5\n6\n7\n8\n\n\n\n\nAltitude in meters\n800\n400\n700\n1000\n1300\n1600\n1200\n900\n\n\n\nFrom this array, the highest altitude in this entire mountain range is 1300 meters. But some travellers may only traverse part of this mountain range; for example, someone that travels from landmark 1 to 4 reaches a maximum altitude of 1000 meters, and another person who travels from landmark 7 to 8 reaches a maximum altitude of 1200 meters. These continuous segments are referred to as subarrays.\nNow imagine that many tourists ask about the highest altitude they can reach by traversing an arbitrary segment of this mountain range. Thus, a method to find the maximum value of any subarray in this measurement array is required, and each question must be answerable in a practically instant amount of time. An initial solution is to build a table with the maximum altitudes reached for every subarray from index \\(X\\) to \\(Y\\) like below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nX = 1\nX = 2\nX = 3\nX = 4\nX = 5\nX = 6\nX = 7\nX = 8\n\n\n\n\nY = 1\n800\n-\n-\n-\n-\n-\n-\n-\n\n\nY = 2\n800\n400\n-\n-\n-\n-\n-\n-\n\n\nY = 3\n800\n700\n700\n-\n-\n-\n-\n-\n\n\nY = 4\n1000\n1000\n1000\n1000\n-\n-\n-\n-\n\n\nY = 5\n1300\n1300\n1300\n1300\n1300\n-\n-\n-\n\n\nY = 6\n1600\n1600\n1600\n1600\n1600\n1600\n-\n-\n\n\nY = 7\n1600\n1600\n1600\n1600\n1600\n1600\n1200\n-\n\n\nY = 8\n1600\n1600\n1600\n1600\n1600\n1600\n1200\n900\n\n\n\nAny tourist’s query can now be answered by instantly looking up the specific subarray in this table. This method is effective, but also tedious because for any positive value \\(C\\), increasing the length of this array by a factor of \\(C\\) will increase the number of subarrays computed by a factor of \\(C^2\\). In this example, all 36 possible subarrays are computed, which is manageable. But suppose on larger scale, a robot took far more measurements on the mountain, such as a million. This table method would then have to compute the maximum value of each of roughly 5 trillion subarrays, which is infeasible even with a computer.\nThere is a more efficient way because not every subarray maximum value has to be literally computed. For example, knowing that maximum values for the subarrays from index 1 to 4 and index 3 to 6 to be 1000 and 1600 respectively is enough to conclude that the maximum value for the subarray from index 1 to 6 must be 1600. This is because every index from 1 to 6 is in at least one of these subarrays, thus the maximum value for the entire subarray must be the higher of 1000 and 1600. This important observation motivates the following sparse table.\nLet \\(L\\) be the length of a subarray. Start by “computing” the maximum value of every single element subarray.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubarray length\nX = 1\nX = 2\nX = 3\nX = 4\nX = 5\nX = 6\nX = 7\nX = 8\n\n\n\n\nL = 1\n800\n400\n700\n1000\n1300\n1600\n1200\n900\n\n\n\nThe maximum of the subarray of length \\(L\\) from index \\(X\\) to \\(X+L-1\\) inclusive is equal to the higher of the maximums for the subarray from index \\(X\\) to \\(X+L/2-1\\) and index \\(X+L/2\\) to \\(X+L-1\\). The computed subarray maximums for \\(L = 1\\) can then be used to compute the subarray maximums for \\(L = 2\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubarray length\nX = 1\nX = 2\nX = 3\nX = 4\nX = 5\nX = 6\nX = 7\nX = 8\n\n\n\n\nL = 1\n800\n400\n700\n1000\n1300\n1600\n1200\n900\n\n\nL = 2\n800\n700\n1000\n1300\n1600\n1600\n1200\n-\n\n\n\nThis information can then be used to compute the subarray maximums for \\(L = 4\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubarray length\nX = 1\nX = 2\nX = 3\nX = 4\nX = 5\nX = 6\nX = 7\nX = 8\n\n\n\n\nL = 1\n800\n400\n700\n1000\n1300\n1600\n1200\n900\n\n\nL = 2\n800\n700\n1000\n1300\n1600\n1600\n1200\n-\n\n\nL = 4\n1000\n1300\n1600\n1600\n1600\n-\n-\n-\n\n\n\nAnd lastly in this case, \\(L = 8\\) (further steps would use larger powers of 2 for \\(L\\)):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubarray length\nX = 1\nX = 2\nX = 3\nX = 4\nX = 5\nX = 6\nX = 7\nX = 8\n\n\n\n\nL = 1\n800\n400\n700\n1000\n1300\n1600\n1200\n900\n\n\nL = 2\n800\n700\n1000\n1300\n1600\n1600\n1200\n-\n\n\nL = 4\n1000\n1300\n1600\n1600\n1600\n-\n-\n-\n\n\nL = 8\n1600\n-\n-\n-\n-\n-\n-\n-\n\n\n\nThe maximum of any subarray from index \\(A\\) to \\(B\\) can then be computed using this sparse table in 3 steps:\n\nCompute \\(N\\), the length of the subarray. (\\(N = B - A + 1\\))\nDetermine \\(M\\), the largest power of 2 (1, 2, 4, 8, 16, 32…) satisfying \\(M \\leq N\\).\nLook in the row \\(L = M\\) and choose the larger of the values in columns \\(X = A\\) and \\(X = B - M + 1\\), representing the subarrays from index \\(A\\) to \\(A + M - 1\\) and index \\(B - M + 1\\) to \\(B\\). These subarrays are guaranteed to cover every index from \\(A\\) to \\(B\\).\n\nAs an example, suppose a tourist wanted to know the highest altitude (maximum) between landmarks 2 to 4. Use the method to determine the maximum of the subarray from index 2 to 4:\n\n\\(N = 4 - 2 + 1\\) =&gt; \\(N = 3\\)\n\\(M = 2\\) (largest power of 2 less than or equal to 3)\nValue in \\(L = 2, X = 2\\) is 700; value in \\(L = 2, X = 4 - 2 + 1 = 3\\) is 1000. Thus the maximum altitude between landmarks 2 to 4 is 1000.\n\nWith the sparse table, only 21 total subarrays have their maximum computed, compared to 36 with the naive table approach. Where this efficiency is more pronounced is with more measurements; for any positive value \\(C\\), increasing the length of the array by a factor of \\(C\\) will now only increase the number of subarrays computed by a factor of \\(C\\text{log}(C)\\). Going back to the robot scenario, if the array consisted of a million values, the number of maximums for subarrays computed would decrease from roughly 5 trillion with a naive table to 19 million with a sparse table."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]